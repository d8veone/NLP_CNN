{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit67420126ac8844db8feb4865fa004feb",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{1: 'film',\n 2: 'one',\n 3: 'movie',\n 4: 'like',\n 5: 'even',\n 6: 'good',\n 7: 'time',\n 8: 'story',\n 9: 'films',\n 10: 'would',\n 11: 'much',\n 12: 'also',\n 13: 'characters',\n 14: 'get',\n 15: 'character',\n 16: 'two',\n 17: 'first',\n 18: 'see',\n 19: 'way',\n 20: 'well',\n 21: 'make',\n 22: 'really',\n 23: 'little',\n 24: 'life',\n 25: 'plot',\n 26: 'people',\n 27: 'bad',\n 28: 'could',\n 29: 'scene',\n 30: 'movies',\n 31: 'never',\n 32: 'best',\n 33: 'new',\n 34: 'scenes',\n 35: 'man',\n 36: 'many',\n 37: 'doesnt',\n 38: 'know',\n 39: 'dont',\n 40: 'hes',\n 41: 'great',\n 42: 'another',\n 43: 'action',\n 44: 'love',\n 45: 'us',\n 46: 'go',\n 47: 'director',\n 48: 'end',\n 49: 'something',\n 50: 'still',\n 51: 'seems',\n 52: 'back',\n 53: 'made',\n 54: 'theres',\n 55: 'work',\n 56: 'makes',\n 57: 'however',\n 58: 'years',\n 59: 'world',\n 60: 'every',\n 61: 'big',\n 62: 'though',\n 63: 'better',\n 64: 'enough',\n 65: 'take',\n 66: 'seen',\n 67: 'around',\n 68: 'performance',\n 69: 'real',\n 70: 'role',\n 71: 'going',\n 72: 'audience',\n 73: 'gets',\n 74: 'isnt',\n 75: 'think',\n 76: 'may',\n 77: 'things',\n 78: 'actually',\n 79: 'look',\n 80: 'last',\n 81: 'funny',\n 82: 'comedy',\n 83: 'almost',\n 84: 'fact',\n 85: 'played',\n 86: 'thing',\n 87: 'nothing',\n 88: 'say',\n 89: 'although',\n 90: 'right',\n 91: 'thats',\n 92: 'come',\n 93: 'since',\n 94: 'find',\n 95: 'script',\n 96: 'plays',\n 97: 'long',\n 98: 'cast',\n 99: 'john',\n 100: 'old',\n 101: 'ever',\n 102: 'comes',\n 103: 'young',\n 104: 'without',\n 105: 'actors',\n 106: 'show',\n 107: 'part',\n 108: 'least',\n 109: 'lot',\n 110: 'takes',\n 111: 'acting',\n 112: 'original',\n 113: 'point',\n 114: 'away',\n 115: 'star',\n 116: 'goes',\n 117: 'quite',\n 118: 'course',\n 119: 'might',\n 120: 'cant',\n 121: 'family',\n 122: 'minutes',\n 123: 'three',\n 124: 'must',\n 125: 'im',\n 126: 'place',\n 127: 'rather',\n 128: 'interesting',\n 129: 'anything',\n 130: 'screen',\n 131: 'guy',\n 132: 'effects',\n 133: 'far',\n 134: 'day',\n 135: 'yet',\n 136: 'watch',\n 137: 'seem',\n 138: 'year',\n 139: 'didnt',\n 140: 'times',\n 141: 'instead',\n 142: 'sense',\n 143: 'picture',\n 144: 'fun',\n 145: 'always',\n 146: 'special',\n 147: 'home',\n 148: 'give',\n 149: 'making',\n 150: 'trying',\n 151: 'bit',\n 152: 'kind',\n 153: 'job',\n 154: 'want',\n 155: 'wife',\n 156: 'series',\n 157: 'american',\n 158: 'becomes',\n 159: 'pretty',\n 160: 'along',\n 161: 'set',\n 162: 'men',\n 163: 'together',\n 164: 'help',\n 165: 'probably',\n 166: 'woman',\n 167: 'become',\n 168: 'actor',\n 169: 'everything',\n 170: 'hard',\n 171: 'hollywood',\n 172: 'money',\n 173: 'given',\n 174: 'gives',\n 175: 'dialogue',\n 176: 'whole',\n 177: 'sure',\n 178: 'high',\n 179: 'black',\n 180: 'watching',\n 181: 'wants',\n 182: 'got',\n 183: 'death',\n 184: 'music',\n 185: 'feel',\n 186: 'perhaps',\n 187: 'play',\n 188: 'moments',\n 189: 'next',\n 190: 'especially',\n 191: 'less',\n 192: 'done',\n 193: 'everyone',\n 194: 'james',\n 195: 'different',\n 196: 'city',\n 197: 'looks',\n 198: 'sex',\n 199: 'simply',\n 200: 'completely',\n 201: 'whose',\n 202: 'reason',\n 203: 'friends',\n 204: 'shows',\n 205: 'rest',\n 206: 'performances',\n 207: 'horror',\n 208: 'case',\n 209: 'father',\n 210: 'left',\n 211: 'theyre',\n 212: 'several',\n 213: 'couple',\n 214: 'looking',\n 215: 'anyone',\n 216: 'entire',\n 217: 'put',\n 218: 'evil',\n 219: 'mind',\n 220: 'ending',\n 221: 'humor',\n 222: 'getting',\n 223: 'lost',\n 224: 'small',\n 225: 'michael',\n 226: 'problem',\n 227: 'shes',\n 228: 'night',\n 229: 'girl',\n 230: 'line',\n 231: 'true',\n 232: 'human',\n 233: 'main',\n 234: 'use',\n 235: 'turns',\n 236: 'begins',\n 237: 'found',\n 238: 'half',\n 239: 'ive',\n 240: 'stars',\n 241: 'either',\n 242: 'later',\n 243: 'thought',\n 244: 'soon',\n 245: 'alien',\n 246: 'town',\n 247: 'based',\n 248: 'name',\n 249: 'someone',\n 250: 'comic',\n 251: 'mother',\n 252: 'certainly',\n 253: 'else',\n 254: 'wrong',\n 255: 'final',\n 256: 'idea',\n 257: 'unfortunately',\n 258: 'school',\n 259: 'believe',\n 260: 'relationship',\n 261: 'friend',\n 262: 'second',\n 263: 'works',\n 264: 'sequence',\n 265: 'tries',\n 266: 'david',\n 267: 'house',\n 268: 'group',\n 269: 'keep',\n 270: 'used',\n 271: 'war',\n 272: 'called',\n 273: 'dead',\n 274: 'named',\n 275: 'often',\n 276: 'playing',\n 277: 'behind',\n 278: 'said',\n 279: 'written',\n 280: 'despite',\n 281: 'tell',\n 282: 'finally',\n 283: 'hand',\n 284: 'youre',\n 285: 'able',\n 286: 'head',\n 287: 'maybe',\n 288: 'turn',\n 289: 'past',\n 290: 'kids',\n 291: 'including',\n 292: 'finds',\n 293: 'seeing',\n 294: 'days',\n 295: 'perfect',\n 296: 'game',\n 297: 'supposed',\n 298: 'mr',\n 299: 'dark',\n 300: 'shot',\n 301: 'nice',\n 302: 'directed',\n 303: 'fight',\n 304: 'book',\n 305: 'lives',\n 306: 'run',\n 307: 'person',\n 308: 'running',\n 309: 'side',\n 310: 'lines',\n 311: 'camera',\n 312: 'starts',\n 313: 'style',\n 314: 'tv',\n 315: 'live',\n 316: 'boy',\n 317: 'car',\n 318: 'moment',\n 319: 'nearly',\n 320: 'face',\n 321: 'worth',\n 322: 'need',\n 323: 'care',\n 324: 'son',\n 325: 'entertaining',\n 326: 'daughter',\n 327: 'upon',\n 328: 'others',\n 329: 'start',\n 330: 'worst',\n 331: 'joe',\n 332: 'try',\n 333: 'full',\n 334: 'video',\n 335: 'example',\n 336: 'exactly',\n 337: 'violence',\n 338: 'opening',\n 339: 'matter',\n 340: 'summer',\n 341: 'hour',\n 342: 'major',\n 343: 'direction',\n 344: 'kevin',\n 345: 'whos',\n 346: 'let',\n 347: 'children',\n 348: 'beautiful',\n 349: 'review',\n 350: 'throughout',\n 351: 'wasnt',\n 352: 'already',\n 353: 'title',\n 354: 'sequences',\n 355: 'problems',\n 356: 'version',\n 357: 'eyes',\n 358: 'early',\n 359: 'short',\n 360: 'robert',\n 361: 'act',\n 362: 'drama',\n 363: 'order',\n 364: 'classic',\n 365: 'team',\n 366: 'fine',\n 367: 'knows',\n 368: 'obvious',\n 369: 'production',\n 370: 'kill',\n 371: 'top',\n 372: 'question',\n 373: 'roles',\n 374: 'boring',\n 375: 'truly',\n 376: 'hit',\n 377: 'screenplay',\n 378: 'guys',\n 379: 'sort',\n 380: 'sometimes',\n 381: 'beginning',\n 382: 'simple',\n 383: 'supporting',\n 384: 'jackie',\n 385: 'earth',\n 386: 'body',\n 387: 'jack',\n 388: 'known',\n 389: 'save',\n 390: 'space',\n 391: 'white',\n 392: 'jokes',\n 393: 'women',\n 394: 'hell',\n 395: 'yes',\n 396: 'deep',\n 397: 'killer',\n 398: 'tells',\n 399: 'novel',\n 400: 'scream',\n 401: 'tom',\n 402: 'coming',\n 403: 'room',\n 404: 'wont',\n 405: 'york',\n 406: 'peter',\n 407: 'particularly',\n 408: 'strong',\n 409: 'extremely',\n 410: 'ends',\n 411: 'four',\n 412: 'saw',\n 413: 'attempt',\n 414: 'manages',\n 415: 'worse',\n 416: 'happens',\n 417: 'genre',\n 418: 'heart',\n 419: 'possible',\n 420: 'girls',\n 421: 'stupid',\n 422: 'five',\n 423: 'sound',\n 424: 'quickly',\n 425: 'romantic',\n 426: 'says',\n 427: 'lead',\n 428: 'thriller',\n 429: 'lee',\n 430: 'result',\n 431: 'wonder',\n 432: 'future',\n 433: 'dog',\n 434: 'arent',\n 435: 'taking',\n 436: 'appears',\n 437: 'hero',\n 438: 'hope',\n 439: 'stop',\n 440: 'murder',\n 441: 'involved',\n 442: 'police',\n 443: 'fiction',\n 444: 'level',\n 445: 'whats',\n 446: 'attention',\n 447: 'close',\n 448: 'involving',\n 449: 'falls',\n 450: 'child',\n 451: 'de',\n 452: 'sets',\n 453: 'van',\n 454: 'hours',\n 455: 'planet',\n 456: 'mostly',\n 457: 'voice',\n 458: 'experience',\n 459: 'career',\n 460: 'fall',\n 461: 'material',\n 462: 'elements',\n 463: 'eventually',\n 464: 'living',\n 465: 'note',\n 466: 'fans',\n 467: 'ship',\n 468: 'emotional',\n 469: 'among',\n 470: 'bring',\n 471: 'ones',\n 472: 'lack',\n 473: 'dr',\n 474: 'wild',\n 475: 'laugh',\n 476: 'obviously',\n 477: 'number',\n 478: 'single',\n 479: 'meet',\n 480: 'chris',\n 481: 'aliens',\n 482: 'happen',\n 483: 'late',\n 484: 'enjoy',\n 485: 'alone',\n 486: 'none',\n 487: 'leads',\n 488: 'youll',\n 489: 'word',\n 490: 'brother',\n 491: 'piece',\n 492: 'taken',\n 493: 'attempts',\n 494: 'husband',\n 495: 'guess',\n 496: 'theater',\n 497: 'chance',\n 498: 'feels',\n 499: 'brothers',\n 500: 'mean',\n 501: 'needs',\n 502: 'laughs',\n 503: 'talent',\n 504: 'talk',\n 505: 'killed',\n 506: 'computer',\n 507: 'leave',\n 508: 'george',\n 509: 'usually',\n 510: 'whether',\n 511: 'battle',\n 512: 'within',\n 513: 'god',\n 514: 'wonderful',\n 515: 'feeling',\n 516: 'oscar',\n 517: 'poor',\n 518: 'across',\n 519: 'interest',\n 520: 'easy',\n 521: 'mission',\n 522: 'deal',\n 523: 'history',\n 524: 'parents',\n 525: 'williams',\n 526: 'science',\n 527: 'words',\n 528: 'call',\n 529: 'features',\n 530: 'told',\n 531: 'feature',\n 532: 'premise',\n 533: 'television',\n 534: 'paul',\n 535: 'somehow',\n 536: 'success',\n 537: 'seemed',\n 538: 'meets',\n 539: 'expect',\n 540: 'basically',\n 541: 'impressive',\n 542: 'form',\n 543: 'tale',\n 544: 'forced',\n 545: 'crew',\n 546: 'stuff',\n 547: 'apparently',\n 548: 'serious',\n 549: 'power',\n 550: 'parts',\n 551: 'except',\n 552: 'recent',\n 553: 'filmmakers',\n 554: 'ryan',\n 555: 'cool',\n 556: 'events',\n 557: 'entertainment',\n 558: 'disney',\n 559: 'easily',\n 560: 'oh',\n 561: 'ben',\n 562: 'score',\n 563: 'working',\n 564: 'giving',\n 565: 'surprise',\n 566: 'difficult',\n 567: 'release',\n 568: 'sequel',\n 569: 'batman',\n 570: 'middle',\n 571: 'released',\n 572: 'reality',\n 573: 'went',\n 574: 'local',\n 575: 'runs',\n 576: 'smith',\n 577: 'hilarious',\n 578: 'robin',\n 579: 'using',\n 580: 'lets',\n 581: 'ago',\n 582: 'king',\n 583: 'happy',\n 584: 'william',\n 585: 'viewers',\n 586: 'brings',\n 587: 'crime',\n 588: 'change',\n 589: 'came',\n 590: 'important',\n 591: 'audiences',\n 592: 'return',\n 593: 'complete',\n 594: 'certain',\n 595: 'turned',\n 596: 'die',\n 597: 'art',\n 598: 'somewhat',\n 599: 'remember',\n 600: 'youve',\n 601: 'ill',\n 602: 'dramatic',\n 603: 'effective',\n 604: 'popular',\n 605: 'strange',\n 606: 'credits',\n 607: 'viewer',\n 608: 'begin',\n 609: 'presence',\n 610: 'suspense',\n 611: 'similar',\n 612: 'wouldnt',\n 613: 'due',\n 614: 'figure',\n 615: 'blood',\n 616: 'surprisingly',\n 617: 'girlfriend',\n 618: 'quality',\n 619: 'ways',\n 620: 'business',\n 621: 'anyway',\n 622: 'couldnt',\n 623: 'beyond',\n 624: 'light',\n 625: 'sexual',\n 626: 'latest',\n 627: 'decides',\n 628: 'rich',\n 629: 'writing',\n 630: 'absolutely',\n 631: 'mystery',\n 632: 'personal',\n 633: 'familiar',\n 634: 'uses',\n 635: 'nature',\n 636: 'brilliant',\n 637: 'cut',\n 638: 'means',\n 639: 'read',\n 640: 'flick',\n 641: 'successful',\n 642: 'amazing',\n 643: 'previous',\n 644: 'jim',\n 645: 'towards',\n 646: 'kid',\n 647: 'predictable',\n 648: 'shots',\n 649: 'intelligent',\n 650: 'gone',\n 651: 'visual',\n 652: 'powerful',\n 653: 'type',\n 654: 'situation',\n 655: 'talking',\n 656: 'annoying',\n 657: 'third',\n 658: 'stories',\n 659: 'felt',\n 660: 'starring',\n 661: 'office',\n 662: 'following',\n 663: 'leaving',\n 664: 'clear',\n 665: 'giant',\n 666: 'cinema',\n 667: 'secret',\n 668: 'excellent',\n 669: 'potential',\n 670: 'red',\n 671: 'add',\n 672: 'romance',\n 673: 'create',\n 674: 'cop',\n 675: 'questions',\n 676: 'actress',\n 677: 'understand',\n 678: 'project',\n 679: 'doubt',\n 680: 'sam',\n 681: 'former',\n 682: 'leaves',\n 683: 'present',\n 684: 'clever',\n 685: 'learn',\n 686: 'move',\n 687: 'writer',\n 688: 'company',\n 689: 'rock',\n 690: 'motion',\n 691: 'thinking',\n 692: 'party',\n 693: 'effect',\n 694: 'id',\n 695: 'trek',\n 696: 'definitely',\n 697: 'huge',\n 698: 'water',\n 699: 'bill',\n 700: 'scary',\n 701: 'seriously',\n 702: 'opens',\n 703: 'age',\n 704: 'married',\n 705: 'general',\n 706: 'america',\n 707: 'follows',\n 708: 'happened',\n 709: 'directors',\n 710: 'bob',\n 711: 'usual',\n 712: 'perfectly',\n 713: 'near',\n 714: 'straight',\n 715: 'saying',\n 716: 'wedding',\n 717: 'large',\n 718: 'unlike',\n 719: 'created',\n 720: 'heard',\n 721: 'sweet',\n 722: 'prison',\n 723: 'solid',\n 724: 'merely',\n 725: 'jones',\n 726: 'smart',\n 727: 'slow',\n 728: 'villain',\n 729: 'likely',\n 730: 'mark',\n 731: 'dumb',\n 732: 'took',\n 733: 'bunch',\n 734: 'various',\n 735: 'realize',\n 736: 'plan',\n 737: 'million',\n 738: 'stay',\n 739: 'message',\n 740: 'subject',\n 741: 'moving',\n 742: 'escape',\n 743: 'ultimately',\n 744: 'country',\n 745: 'enjoyable',\n 746: 'scott',\n 747: 'agent',\n 748: 'break',\n 749: 'decent',\n 750: 'force',\n 751: 'impossible',\n 752: 'points',\n 753: 'fails',\n 754: 'mess',\n 755: 'follow',\n 756: 'tim',\n 757: 'immediately',\n 758: 'appear',\n 759: 'political',\n 760: 'exciting',\n 761: 'private',\n 762: 'brought',\n 763: 'filled',\n 764: 'pay',\n 765: 'harry',\n 766: 'murphy',\n 767: 'animated',\n 768: 'soundtrack',\n 769: 'dream',\n 770: 'favorite',\n 771: 'overall',\n 772: 'cold',\n 773: 'truth',\n 774: 'spend',\n 775: 'mars',\n 776: 'members',\n 777: 'biggest',\n 778: 'inside',\n 779: 'trouble',\n 780: 'element',\n 781: 'keeps',\n 782: 'fan',\n 783: 'effort',\n 784: 'eddie',\n 785: 'studio',\n 786: 'particular',\n 787: 'talented',\n 788: 'english',\n 789: 'hands',\n 790: 'focus',\n 791: 'neither',\n 792: 'silly',\n 793: 'liked',\n 794: 'eye',\n 795: 'carter',\n 796: 'bond',\n 797: 'slightly',\n 798: 'credit',\n 799: 'constantly',\n 800: 'offers',\n 801: 'musical',\n 802: 'richard',\n 803: 'otherwise',\n 804: 'actual',\n 805: 'cannot',\n 806: 'havent',\n 807: 'drug',\n 808: 'purpose',\n 809: 'martin',\n 810: 'earlier',\n 811: 'ideas',\n 812: 'ten',\n 813: 'rating',\n 814: 'wars',\n 815: 'totally',\n 816: 'thinks',\n 817: 'memorable',\n 818: 'bruce',\n 819: 'chase',\n 820: 'box',\n 821: 'open',\n 822: 'brief',\n 823: 'british',\n 824: 'wait',\n 825: 'ask',\n 826: 'wanted',\n 827: 'view',\n 828: 'waste',\n 829: 'atmosphere',\n 830: 'entirely',\n 831: 'control',\n 832: 'steve',\n 833: 'cinematography',\n 834: 'fast',\n 835: 'humans',\n 836: 'gun',\n 837: 'greatest',\n 838: 'suddenly',\n 839: 'lots',\n 840: 'showing',\n 841: 'situations',\n 842: 'state',\n 843: 'law',\n 844: 'disaster',\n 845: 'frank',\n 846: 'subplot',\n 847: 'aspect',\n 848: 'ability',\n 849: 'critics',\n 850: 'gave',\n 851: 'terrible',\n 852: 'modern',\n 853: 'spent',\n 854: 'society',\n 855: 'wish',\n 856: 'moves',\n 857: 'army',\n 858: 'air',\n 859: 'fear',\n 860: 'setting',\n 861: 'park',\n 862: 'sit',\n 863: 'fairly',\n 864: 'female',\n 865: 'animation',\n 866: 'nick',\n 867: 'government',\n 868: 'mary',\n 869: 'max',\n 870: 'hear',\n 871: 'sees',\n 872: 'expected',\n 873: 'approach',\n 874: 'song',\n 875: 'steven',\n 876: 'outside',\n 877: 'ridiculous',\n 878: 'depth',\n 879: 'west',\n 880: 'tension',\n 881: 'whatever',\n 882: 'class',\n 883: 'amount',\n 884: 'street',\n 885: 'typical',\n 886: 'list',\n 887: 'killing',\n 888: 'indeed',\n 889: 'dull',\n 890: 'tone',\n 891: 'cameron',\n 892: 'recently',\n 893: 'screenwriter',\n 894: 'ii',\n 895: 'boys',\n 896: 'clearly',\n 897: 'trailer',\n 898: 'stand',\n 899: 'hate',\n 900: 'imagine',\n 901: 'cheap',\n 902: 'teen',\n 903: 'quick',\n 904: 'minor',\n 905: 'plenty',\n 906: 'provide',\n 907: 'sister',\n 908: 'background',\n 909: 'violent',\n 910: 'shown',\n 911: 'hold',\n 912: 'island',\n 913: 'subtle',\n 914: 'free',\n 915: 'convincing',\n 916: 'college',\n 917: 'joke',\n 918: 'miss',\n 919: 'stone',\n 920: 'ride',\n 921: 'dreams',\n 922: 'meanwhile',\n 923: 'titanic',\n 924: 'knew',\n 925: 'awful',\n 926: 'front',\n 927: 'common',\n 928: 'charm',\n 929: 'possibly',\n 930: 'fire',\n 931: 'basic',\n 932: 'complex',\n 933: 'key',\n 934: 'provides',\n 935: 'images',\n 936: 'scifi',\n 937: 'woody',\n 938: 'okay',\n 939: 'puts',\n 940: 'amusing',\n 941: 'chan',\n 942: 'appearance',\n 943: 'carry',\n 944: 'theme',\n 945: 'sight',\n 946: 'grace',\n 947: 'detective',\n 948: 'incredibly',\n 949: 'language',\n 950: 'leading',\n 951: 'beauty',\n 952: 'sounds',\n 953: 'flat',\n 954: 'somewhere',\n 955: 'telling',\n 956: 'master',\n 957: 'impact',\n 958: 'cinematic',\n 959: 'la',\n 960: 'opportunity',\n 961: 'development',\n 962: 'famous',\n 963: 'road',\n 964: 'writers',\n 965: 'sent',\n 966: 'reasons',\n 967: 'forget',\n 968: 'french',\n 969: 'lies',\n 970: 'seven',\n 971: 'truman',\n 972: 'tarzan',\n 973: 'mysterious',\n 974: 'remains',\n 975: 'consider',\n 976: 'trip',\n 977: 'climax',\n 978: 'terrific',\n 979: 'considering',\n 980: 'sean',\n 981: 'decide',\n 982: 'mike',\n 983: 'willis',\n 984: 'ready',\n 985: 'rated',\n 986: 'delivers',\n 987: 'realistic',\n 988: 'highly',\n 989: 'aside',\n 990: 'proves',\n 991: 'longer',\n 992: 'etc',\n 993: 'believable',\n 994: 'powers',\n 995: 'member',\n 996: 'average',\n 997: 'chemistry',\n 998: 'thanks',\n 999: 'seemingly',\n 1000: 'pictures',\n ...}"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b9c5194572b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate max document length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# calculate vocabulary size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Max document length: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-86d55cdec380>\u001b[0m in \u001b[0;36mmax_length\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate the maximum document length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-86d55cdec380>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# calculate the maximum document length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    " \n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n",
    " \n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model\n",
    " \n",
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)\n",
    " \n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}