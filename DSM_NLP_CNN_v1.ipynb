{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bit67420126ac8844db8feb4865fa004feb",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "trainLines, trainLabels = load_dataset('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "535"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainLines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "44276"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate max document length\n",
    "length = max_length(trainLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1380"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Max document length: 1380\nVocabulary size: 44277\n"
    }
   ],
   "source": [
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(1800, 1380)\n"
    }
   ],
   "source": [
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[   27,    27,    27, ...,     0,     0,     0],\n       [   74,  1536,  1426, ...,     0,     0,     0],\n       [ 7430,     3, 16201, ...,     0,     0,     0],\n       [19619,  1517,   230, ...,     0,     0,     0],\n       [  360,  4574,   237, ...,     0,     0,     0]], dtype=int32)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\t#plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_9\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_25 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\ninput_26 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\ninput_27 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\nembedding_25 (Embedding)        (None, 1380, 100)    4427700     input_25[0][0]                   \n__________________________________________________________________________________________________\nembedding_26 (Embedding)        (None, 1380, 100)    4427700     input_26[0][0]                   \n__________________________________________________________________________________________________\nembedding_27 (Embedding)        (None, 1380, 100)    4427700     input_27[0][0]                   \n__________________________________________________________________________________________________\nconv1d_25 (Conv1D)              (None, 1377, 32)     12832       embedding_25[0][0]               \n__________________________________________________________________________________________________\nconv1d_26 (Conv1D)              (None, 1375, 32)     19232       embedding_26[0][0]               \n__________________________________________________________________________________________________\nconv1d_27 (Conv1D)              (None, 1373, 32)     25632       embedding_27[0][0]               \n__________________________________________________________________________________________________\ndropout_25 (Dropout)            (None, 1377, 32)     0           conv1d_25[0][0]                  \n__________________________________________________________________________________________________\ndropout_26 (Dropout)            (None, 1375, 32)     0           conv1d_26[0][0]                  \n__________________________________________________________________________________________________\ndropout_27 (Dropout)            (None, 1373, 32)     0           conv1d_27[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling1d_25 (MaxPooling1D) (None, 688, 32)      0           dropout_25[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling1d_26 (MaxPooling1D) (None, 687, 32)      0           dropout_26[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling1d_27 (MaxPooling1D) (None, 686, 32)      0           dropout_27[0][0]                 \n__________________________________________________________________________________________________\nflatten_25 (Flatten)            (None, 22016)        0           max_pooling1d_25[0][0]           \n__________________________________________________________________________________________________\nflatten_26 (Flatten)            (None, 21984)        0           max_pooling1d_26[0][0]           \n__________________________________________________________________________________________________\nflatten_27 (Flatten)            (None, 21952)        0           max_pooling1d_27[0][0]           \n__________________________________________________________________________________________________\nconcatenate_9 (Concatenate)     (None, 65952)        0           flatten_25[0][0]                 \n                                                                 flatten_26[0][0]                 \n                                                                 flatten_27[0][0]                 \n__________________________________________________________________________________________________\ndense_17 (Dense)                (None, 10)           659530      concatenate_9[0][0]              \n__________________________________________________________________________________________________\ndense_18 (Dense)                (None, 1)            11          dense_17[0][0]                   \n==================================================================================================\nTotal params: 14,000,337\nTrainable params: 14,000,337\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_16 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\ninput_17 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\ninput_18 (InputLayer)           (None, 1380)         0                                            \n__________________________________________________________________________________________________\nembedding_16 (Embedding)        (None, 1380, 100)    4427700     input_16[0][0]                   \n__________________________________________________________________________________________________\nembedding_17 (Embedding)        (None, 1380, 100)    4427700     input_17[0][0]                   \n__________________________________________________________________________________________________\nembedding_18 (Embedding)        (None, 1380, 100)    4427700     input_18[0][0]                   \n__________________________________________________________________________________________________\nconv1d_16 (Conv1D)              (None, 1377, 32)     12832       embedding_16[0][0]               \n__________________________________________________________________________________________________\nconv1d_17 (Conv1D)              (None, 1375, 32)     19232       embedding_17[0][0]               \n__________________________________________________________________________________________________\nconv1d_18 (Conv1D)              (None, 1373, 32)     25632       embedding_18[0][0]               \n__________________________________________________________________________________________________\ndropout_16 (Dropout)            (None, 1377, 32)     0           conv1d_16[0][0]                  \n__________________________________________________________________________________________________\ndropout_17 (Dropout)            (None, 1375, 32)     0           conv1d_17[0][0]                  \n__________________________________________________________________________________________________\ndropout_18 (Dropout)            (None, 1373, 32)     0           conv1d_18[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling1d_16 (MaxPooling1D) (None, 688, 32)      0           dropout_16[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling1d_17 (MaxPooling1D) (None, 687, 32)      0           dropout_17[0][0]                 \n__________________________________________________________________________________________________\nmax_pooling1d_18 (MaxPooling1D) (None, 686, 32)      0           dropout_18[0][0]                 \n__________________________________________________________________________________________________\nflatten_16 (Flatten)            (None, 22016)        0           max_pooling1d_16[0][0]           \n__________________________________________________________________________________________________\nflatten_17 (Flatten)            (None, 21984)        0           max_pooling1d_17[0][0]           \n__________________________________________________________________________________________________\nflatten_18 (Flatten)            (None, 21952)        0           max_pooling1d_18[0][0]           \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 65952)        0           flatten_16[0][0]                 \n                                                                 flatten_17[0][0]                 \n                                                                 flatten_18[0][0]                 \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 10)           659530      concatenate_6[0][0]              \n__________________________________________________________________________________________________\ndense_12 (Dense)                (None, 1)            11          dense_11[0][0]                   \n==================================================================================================\nTotal params: 14,000,337\nTrainable params: 14,000,337\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\nEpoch 1/10\n1800/1800 [==============================] - 69s 38ms/step - loss: 0.6916 - accuracy: 0.5311\nEpoch 2/10\n1800/1800 [==============================] - 95s 53ms/step - loss: 0.4841 - accuracy: 0.7500\nEpoch 3/10\n1800/1800 [==============================] - 83s 46ms/step - loss: 0.0603 - accuracy: 0.9800\nEpoch 4/10\n1800/1800 [==============================] - 69s 38ms/step - loss: 0.0033 - accuracy: 1.0000\nEpoch 5/10\n1800/1800 [==============================] - 78s 43ms/step - loss: 9.4909e-04 - accuracy: 1.0000\nEpoch 6/10\n1800/1800 [==============================] - 64s 36ms/step - loss: 4.5758e-04 - accuracy: 1.0000\nEpoch 7/10\n1800/1800 [==============================] - 69s 38ms/step - loss: 3.0495e-04 - accuracy: 1.0000\nEpoch 8/10\n1800/1800 [==============================] - 71s 40ms/step - loss: 2.2254e-04 - accuracy: 1.0000\nEpoch 9/10\n1800/1800 [==============================] - 81s 45ms/step - loss: 1.5897e-04 - accuracy: 1.0000\nEpoch 10/10\n1800/1800 [==============================] - 79s 44ms/step - loss: 1.3086e-04 - accuracy: 1.0000\n"
    }
   ],
   "source": [
    "import pydot\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "# fit model\n",
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=10, batch_size=16)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Max document length: 1380\nVocabulary size: 44277\n(1800, 1380) (200, 1380)\nTrain Accuracy: 100.000000\nTest Accuracy: 86.500001\n"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    " \n",
    "\n",
    " \n",
    "# load datasets\n",
    "trainLines, trainLabels = load_dataset('train.pkl')\n",
    "testLines, testLabels = load_dataset('test.pkl')\n",
    " \n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "testX = encode_text(tokenizer, testLines, length)\n",
    "print(trainX.shape, testX.shape)\n",
    " \n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    " \n",
    "# evaluate model on training dataset\n",
    "loss, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
    "print('Train Accuracy: %f' % (acc*100))\n",
    " \n",
    "# evaluate model on test dataset dataset\n",
    "loss, acc = model.evaluate([testX,testX,testX],array(testLabels), verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}